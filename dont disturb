import os
import tempfile
import pandas as pd
import plotly.express as px
import streamlit as st
import speech_recognition as sr
from langchain.prompts import PromptTemplate
from langchain.chains.question_answering import load_qa_chain
from langchain_community.chat_models import ChatOpenAI
from langchain_openai.embeddings import OpenAIEmbeddings
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain.vectorstores.faiss import FAISS
from prophet import Prophet
from statsmodels.tsa.arima.model import ARIMA
from langchain_experimental.agents import create_pandas_dataframe_agent
from langchain.schema import Document

# Set OpenAI API Key
os.environ["OPENAI_API_KEY"] = "YOUR_OPENAI_API_KEY"  # Replace with your actual API key

# Add custom CSS for styling
st.markdown(
    """
    <style>
    body {
        background: linear-gradient(to right, #e49cb4, #ffdfd3);
        color: #ffe3fd;
    }
    .sidebar .sidebar-content {
        background-color: #f9c3d9;  /* Pastel pink */
        color: #2c3e50;  /* Darker text for contrast */
        font-family: 'Arial', sans-serif;  /
        margin: 5px 0;
        color: white;* Nice font */
    }
    .stRadio > div {
        background-color: #ffb3b3;  /* Highlighted background for radio buttons */
        border-radius: 5px;
        padding: 10px;
        border: None;
    }
    .stRadio > div:hover {
        background-color: #ffe3fd;  /* Darker pink on hover */
    }
    .stButton>button {
        background-color: #7f3667;
        border-radius: 5px;
        padding: 10px 20px;
        cursor: pointer;
        transition: background-color 0.3s;
    }
    .stButton>button:hover {
        background-color: #f9dbe6;
    }
    h1 {
        text-align: center;
        color: #bb437e;
    }
    </style>
    """,
    unsafe_allow_html=True
)

@st.cache_data
def load_csv_as_documents(file_path):
    """Load CSV data and convert it into a list of documents."""
    try:
        df = pd.read_csv(file_path, na_values=["", "NA", "NaN"])
        documents = [
            Document(page_content=" ".join(row.fillna("").astype(str))) for _, row in df.iterrows()
        ]
        return df, documents
    except Exception as e:
        st.error(f"Error loading CSV: {e}")
        return None, []

@st.cache_resource
def create_embeddings_and_retriever(_documents):
    """Create embeddings and retriever for document search using FAISS."""
    embeddings = OpenAIEmbeddings()
    text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=200)
    chunks = [chunk for doc in _documents for chunk in text_splitter.split_text(doc.page_content)]
    faiss_index = FAISS.from_texts(chunks, embeddings)
    return faiss_index.as_retriever()

def get_model_response(documents, query):
    """Generate a response using LangChain and OpenAI."""
    try:
        retriever = create_embeddings_and_retriever(documents)
        relevant_docs = retriever.get_relevant_documents(query)

        prompt_template = """
            Given the following context and a question, generate an answer based on this context only.
            If the answer is not found in the context, state "I don't know."
            Context: {context}
            Question: {question}
            Answer:
        """
        prompt = PromptTemplate(template=prompt_template, input_variables=["context", "question"])
        model = ChatOpenAI(temperature=0.1)
        chain = load_qa_chain(model, chain_type="stuff", prompt=prompt)

        response = chain.invoke({"input_documents": relevant_docs, "question": query})
        return response.get('output_text', "I don't know.")
    except Exception as e:
        st.error(f"Error generating response: {e}")
        return "I don't know."

@st.cache_resource
def create_dataframe_agent(df):
    """Create a LangChain pandas DataFrame agent."""
    llm = ChatOpenAI(model_name="gpt-4", temperature=0)
    return create_pandas_dataframe_agent(llm, df, verbose=False, allow_dangerous_code=True)

def generate_plot_from_prompt(df, prompt):
    try:
        # Normalize column names for consistent AI inference
        df.columns = df.columns.str.strip().str.lower().str.replace(r'\s+', '_', regex=True)

        # Track missing values by column
        missing_values = {}

        # Provide dataset context to AI
        column_info = f"""
        The dataset contains the following columns: {', '.join(df.columns)}.
        Here are some example rows:
        {df.head(3).to_string(index=False)}
        """
        detailed_prompt = f"""
        {prompt}

        {column_info}

        Based on this dataset, determine:
        1. The type of plot (scatter, bar, line, histogram, boxplot, pie, etc.).
        2. The column(s) required for the plot.
        3. Optional axis labels, if necessary.

        Respond in the following format:
        Plot type: [plot_type]
        Column(s): [column1, column2] (if applicable)
        X-axis label: [label] (optional)
        Y-axis label: [label] (optional)
        """

        # Create agent and get AI response
        agent = create_dataframe_agent(df)
        response = agent.run(detailed_prompt)
        st.write("AI Response:", response)

        # Check if the agent is recommending plotting missing values
        if "missing values" in response.lower():
            # Handle missing values
            missing_data = df.isnull().sum()
            fig = px.bar(x=missing_data.index, y=missing_data.values, title="Missing Values Count")
            fig.update_layout(xaxis_title="Column Names", yaxis_title="Count of Missing Values")
            st.plotly_chart(fig)
            return

        # Extract plot details from the AI response
        plot_type, columns, x_label, y_label = None, [], None, None
        if response:
            response_lower = response.lower()

            # Extract plot type
            for p_type in ["scatter", "line", "bar", "histogram", "boxplot", "pie"]:
                if p_type in response_lower:
                    plot_type = p_type
                    break

            # Extract columns
            inferred_columns = [col for col in df.columns if col in response_lower]
            if inferred_columns:
                columns = inferred_columns

            # Extract axis labels
            if "x-axis label:" in response_lower:
                x_label_start = response_lower.find("x-axis label:") + len("x-axis label:")
                x_label = response_lower[x_label_start:].splitlines()[0].strip()

            if "y-axis label:" in response_lower:
                y_label_start = response_lower.find("y-axis label:") + len("y-axis label:")
                y_label = response_lower[y_label_start:].splitlines()[0].strip()

        # Fallback to manual input if AI response is incomplete
        if not plot_type or not columns:
            st.warning("AI could not generate a complete response. Please specify plot details manually.")
            plot_type = st.selectbox("Select Plot Type", ["scatter", "line", "bar", "histogram", "boxplot", "pie"])
            columns = st.multiselect("Select Column(s)", df.columns)
            x_label = st.text_input("X-axis Label (Optional)")
            y_label = st.text_input("Y-axis Label (Optional)")

        # Validate plot details
        if not plot_type or not columns:
            st.error("Plot details are incomplete. Please specify the type and columns.")
            return

        # Handle missing values in selected columns
        for col in columns:
            if df[col].isnull().sum() > 0:
                st.warning(f"Column '{col}' contains missing values. They will be dropped.")
                missing_values[col] = df[df[col].isnull()]
                df = df.dropna(subset=[col])

        # Generate the plot dynamically using plotly for interactivity
        if plot_type == "scatter" and len(columns) >= 2:
            fig = px.scatter(df, x=columns[0], y=columns[1], title=f"Scatter Plot of {columns[0]} vs {columns[1]}")

        elif plot_type == "bar" and len(columns) >= 2:
            fig = px.bar(df, x=columns[0], y=columns[1], title=f"Bar Plot of {columns[0]} vs {columns[1]}")

        elif plot_type == "line" and len(columns) >= 2:
            fig = px.line(df, x=columns[0], y=columns[1], title=f"Line Plot of {columns[0]} vs {columns[1]}")

        elif plot_type == "histogram" and len(columns) == 1:
            fig = px.histogram(df, x=columns[0], title=f"Histogram of {columns[0]}")

        elif plot_type == "boxplot" and len(columns) >= 2:
            fig = px.box(df, x=columns[0], y=columns[1], title=f"Boxplot of {columns[0]} vs {columns[1]}")
        elif plot_type == "boxplot" and len(columns) == 1:
            fig = px.box(df, y=columns[0], title=f"Boxplot of {columns[0]}")

        elif plot_type == "pie" and len(columns) == 1:
            pie_data = df[columns[0]].value_counts()
            fig = px.pie(names=pie_data.index, values=pie_data, title=f"Pie Chart of {columns[0]}")

        else:
            st.error("Invalid plot details. Please try again.")
            return

        # Apply axis labels if provided
        if x_label:
            fig.update_layout(xaxis_title=x_label)
        if y_label:
            fig.update_layout(yaxis_title=y_label)

        # Show plot
        st.plotly_chart(fig)

        # Store missing values in session state for later use
        if missing_values:
            st.session_state['missing_values'] = missing_values

    except Exception as e:
        st.error(f"Error generating plot: {e}")

def forecast_with_summary(df, user_query):
    try:
        # Extract date and target value columns
        date_col = st.selectbox("Select the date column", df.columns)
        value_col = st.selectbox("Select the target column for forecasting", df.columns)

        df = df[[date_col, value_col]].dropna()
        df[date_col] = pd.to_datetime(df[date_col])
        df = df.sort_values(by=date_col)

        st.write("Data used for forecasting:")
        st.dataframe(df.head())

        # Fit Prophet model
        df.rename(columns={date_col: "ds", value_col: "y"}, inplace=True)
        model = Prophet()
        model.fit(df)

        periods = st.slider("Select forecast horizon (in days)", min_value=1, max_value=365, value=30)
        future = model.make_future_dataframe(periods=periods)
        forecast = model.predict(future)

        # Visualize the forecast
        fig = px.line(forecast, x="ds", y="yhat", title="Forecast", labels={"ds": "Date", "yhat": "Forecast"})
        st.plotly_chart(fig)

        # Summary of forecast
        summary = f"""
        Forecast generated using the {value_col} column over a period of {periods} days.
        The dataset contained {len(df)} rows with data from {df['ds'].min()} to {df['ds'].max()}.
        """
        st.write("Summary:")
        st.write(summary)

        # Allow user to refine with ARIMA
        if st.checkbox("Run ARIMA for comparison"):
            arima_order = (st.number_input("ARIMA p", value=1), st.number_input("ARIMA d", value=1), st.number_input("ARIMA q", value=0))
            arima_model = ARIMA(df["y"], order=arima_order).fit()
            arima_forecast = arima_model.forecast(steps=periods)

            st.write("ARIMA Forecast (first 10 values):")
            st.write(arima_forecast[:10])
            st.line_chart(arima_forecast)

    except Exception as e:
        st.error(f"Error during forecasting: {e}")

# Function to capture voice input with increased listening time
def capture_voice_input():
    recognizer = sr.Recognizer()
    with sr.Microphone() as source:
        st.write("Listening for your command...")

        # Adjusting the timeout and phrase_time_limit
        try:
            audio = recognizer.listen(source, timeout=10, phrase_time_limit=20)
        except sr.WaitTimeoutError:
            st.write("Listening timed out. Please try again.")
            return ""

        try:
            command = recognizer.recognize_google(audio)
            st.write(f"Voice command received: {command}")
            return command
        except sr.UnknownValueError:
            st.write("Sorry, I could not understand the audio.")
            return ""
        except sr.RequestError:
            st.write("Sorry, there was an issue with the speech service.")
            return ""

def main():
    st.title("Executive Summary Tool")  # Updated title

    uploaded_file = st.sidebar.file_uploader("Choose a CSV file", type="csv")

    # Track the current mode using session state
    if 'mode' not in st.session_state:
        st.session_state.mode = None
    if 'query' not in st.session_state:
        st.session_state.query = ""

    # Handle mode change with icons
    mode = st.sidebar.radio("Choose Mode",
                            ["ðŸ¤– Chat with CSV Data", "ðŸ“Š Query DataFrame Directly", "ðŸ“ˆ Visualize Data", "ðŸ”® Forecasting Mode"])

    # Reset the query input when the mode changes
    if st.session_state.mode != mode:
        st.session_state.query = ""  # Reset query
        st.session_state.mode = mode  # Update the mode

    # Handle the file and mode if file is uploaded
    if uploaded_file is not None:
        try:
            with tempfile.NamedTemporaryFile(delete=False) as tmp_file:
                tmp_file.write(uploaded_file.getvalue())
                tmp_file_path = tmp_file.name

            df, documents = load_csv_as_documents(tmp_file_path)

            if df is None or not len(documents):
                st.error("No valid documents found. Please check your CSV format.")
                return

            st.write("Preview of uploaded file:")
            st.dataframe(df.head())

            # Text input for the query
            text_input = st.text_input("Your Query (Type here):", value=st.session_state.query)

            # Create a microphone icon for voice input
            voice_command = None
            if st.button("ðŸ”Š", key="voice-btn", help="Click the microphone icon to use voice command"):
                voice_command = capture_voice_input()

            # Choose query based on voice or text input
            user_input = voice_command if voice_command else text_input
            st.session_state.query = user_input  # Update session state with user input

            if mode == "ðŸ¤– Chat with CSV Data":
                if user_input:
                    with st.spinner("Processing... Please wait."):
                        response = get_model_response(documents, user_input)
                    st.write("Response:")
                    st.write(response)
                else:
                    st.write("Please type a query or use voice command.")

            elif mode == "ðŸ“Š Query DataFrame Directly":
                dataframe_agent = create_dataframe_agent(df)
                if user_input:
                    with st.spinner("Processing... Please wait."):
                        response = dataframe_agent.run(user_input)
                    st.write("Response:")
                    st.write(response)
                else:
                    st.write("Please type a query or use voice command.")

            elif mode == "ðŸ“ˆ Visualize Data":
                if user_input:
                    with st.spinner("Processing... Please wait."):
                        generate_plot_from_prompt(df, user_input)
                else:
                    st.write("Please type a query or use voice command.")

            elif mode == "ðŸ”® Forecasting Mode":
                if user_input:
                    with st.spinner("Processing... Please wait."):
                        forecast_with_summary(df, user_input)
                else:
                    st.write("Please type a query or use voice command.")

        except Exception as e:
            st.error(f"Error processing file: {e}")
    else:
        st.info("Upload a CSV file to get started.")

if _name_ == "_main_":
Â Â Â Â main()
